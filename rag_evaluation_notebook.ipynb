{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Rag Evaluation package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install package\n",
        "%pip install -q --upgrade rag_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import package\n",
        "import rag_evaluation as rag_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set API Key "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can set in Environment / .env or \n",
        "\n",
        "# One-time, in-memory key (per Python session)\n",
        "rag_eval.set_api_key(\"openai\", \"sk-###\")\n",
        "rag_eval.set_api_key(\"gemini\", \"AIza####\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage\n",
        "#### For API-based Models (GPT and Gemini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the the three inputs needed for any RAG evaluation\n",
        "\n",
        "wikipedia_text = \"\"\"\n",
        "A large language model (LLM) is a machine learning model designed for natural language processing tasks, especially language generation.\n",
        "LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
        "\n",
        "The largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT or Gemini.\n",
        "LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and\n",
        "ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "question = \"Which large language model is currently the largest and most capable?\"\n",
        "\n",
        "\n",
        "llm_response = \"\"\"\n",
        "The largest and most capable LLMs are the generative pretrained transformers (GPTs). These models are\n",
        "designed to handle complex language tasks, and their vast number of parameters gives them the ability to\n",
        "understand and generate human-like text.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI Evaluation\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             Metric  Score (Normalized)  Score (%)\n",
            "0   Query Relevance                1.00      100.0\n",
            "1  Factual Accuracy                1.00      100.0\n",
            "2          Coverage                0.80       80.0\n",
            "3         Coherence                1.00      100.0\n",
            "4           Fluency                1.00      100.0\n",
            "5  Overall Accuracy                0.95       95.0\n",
            "\n",
            " \n",
            "  Gemini Evaluation\n",
            "             Metric  Score (Normalized)  Score (%)\n",
            "0   Query Relevance                 1.0      100.0\n",
            "1  Factual Accuracy                 1.0      100.0\n",
            "2          Coverage                 0.8       80.0\n",
            "3         Coherence                 1.0      100.0\n",
            "4           Fluency                 1.0      100.0\n",
            "5  Overall Accuracy                 0.9       90.0\n"
          ]
        }
      ],
      "source": [
        "from rag_evaluation.evaluator import evaluate_response\n",
        "\n",
        "# OpenAI usage \n",
        "print('OpenAI Evaluation')\n",
        "report = evaluate_response(\n",
        "    query=question,\n",
        "    response=llm_response,\n",
        "    document=wikipedia_text,\n",
        "    model_type=\"openai\",\n",
        "    model_name='gpt-4.1',\n",
        ")\n",
        "print(report)\n",
        "\n",
        "# Gemini usage \n",
        "print('\\n \\n  Gemini Evaluation')\n",
        "report = evaluate_response(\n",
        "    query=question,\n",
        "    response=llm_response,\n",
        "    document=wikipedia_text,\n",
        "    model_type=\"gemini\",\n",
        "    model_name='gemini-2.5-flash',\n",
        "    metric_weights=[0.1, 0.4, 0.5, 0., 0.] # optional metric_weights [Query Relevance, Factual Accuracy, Coverage, Coherence, Fluency]\n",
        ")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate directly from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Score (Normalized)</th>\n",
              "      <th>Score (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Query Relevance</td>\n",
              "      <td>0.60</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Factual Accuracy</td>\n",
              "      <td>0.50</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Coverage</td>\n",
              "      <td>0.50</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Coherence</td>\n",
              "      <td>0.60</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fluency</td>\n",
              "      <td>1.00</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Overall Accuracy</td>\n",
              "      <td>0.51</td>\n",
              "      <td>51.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Metric  Score (Normalized)  Score (%)\n",
              "0   Query Relevance                0.60       60.0\n",
              "1  Factual Accuracy                0.50       50.0\n",
              "2          Coverage                0.50       50.0\n",
              "3         Coherence                0.60       60.0\n",
              "4           Fluency                1.00      100.0\n",
              "5  Overall Accuracy                0.51       51.0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd \n",
        "from rag_evaluation import evaluate_df\n",
        "\n",
        "#load your file\n",
        "df = pd.read_csv(\"rag_evaluation_data.csv\")\n",
        "report = evaluate_df(df,\n",
        "                     model_type=\"openai\",\n",
        "                     model_name=\"gpt-4o\",\n",
        "                     metric_weights=[0.1, 0.4, 0.5, 0., 0.]\n",
        "                     )\n",
        "report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Usage with Open-Source Models (Ollama models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama Evaluation\n",
            "             Metric  Score (Normalized)  Score (%)\n",
            "0   Query Relevance                0.20       20.0\n",
            "1  Factual Accuracy                0.80       80.0\n",
            "2          Coverage                0.80       80.0\n",
            "3         Coherence                0.80       80.0\n",
            "4           Fluency                0.80       80.0\n",
            "5  Overall Accuracy                0.65       65.0\n",
            "\n",
            " \n",
            "  Mistral Evaluation\n",
            "             Metric  Score (Normalized)  Score (%)\n",
            "0   Query Relevance                1.00      100.0\n",
            "1  Factual Accuracy                1.00      100.0\n",
            "2          Coverage                0.80       80.0\n",
            "3         Coherence                0.80       80.0\n",
            "4           Fluency                1.00      100.0\n",
            "5  Overall Accuracy                0.82       82.0\n",
            "\n",
            " \n",
            "  Qwen Evaluation\n",
            "             Metric  Score (Normalized)  Score (%)\n",
            "0   Query Relevance                 0.8       80.0\n",
            "1  Factual Accuracy                 0.8       80.0\n",
            "2          Coverage                 0.8       80.0\n",
            "3         Coherence                 0.8       80.0\n",
            "4           Fluency                 0.8       80.0\n",
            "5  Overall Accuracy                 0.8       80.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the inputs\n",
        "query = \"Which large language model is currently the largest and most capable?\"\n",
        "\n",
        "response_text = \"\"\"The largest and most capable LLMs are the generative pretrained transformers (GPTs). These models are \n",
        "                designed to handle complex language tasks, and their vast number of parameters gives them the ability to \n",
        "                understand and generate human-like text.\"\"\"\n",
        "                 \n",
        "document = \"\"\"A large language model (LLM) is a type of machine learning model designed for natural language processing \n",
        "            tasks such as language generation. LLMs are language models with many parameters, and are trained with \n",
        "            self-supervised learning on a vast amount of text. The largest and most capable LLMs are \n",
        "            generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided \n",
        "            by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies \n",
        "            inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\"\"\"\n",
        "\n",
        "# Llama usage ('ollama pull llama3.2:1b' to download from terminal)\n",
        "print('Llama Evaluation')\n",
        "report = evaluate_response(\n",
        "    query=query,\n",
        "    response=response_text,\n",
        "    document=document,\n",
        "    model_type=\"ollama\",\n",
        "    model_name='llama3.2:1b'\n",
        ")\n",
        "print(report)\n",
        "\n",
        "# Mistral usage ('ollama pull mistral' to download from terminal)\n",
        "print('\\n \\n  Mistral Evaluation')\n",
        "report = evaluate_response(\n",
        "    query=query,\n",
        "    response=response_text,\n",
        "    document=document,\n",
        "    model_type=\"ollama\",\n",
        "    model_name='mistral:latest',\n",
        "    metric_weights=[0.1, 0., 0.9, 0., 0.] # optional metric_weights [Query Relevance, Factual Accuracy, Coverage, Coherence, Fluency]\n",
        ")\n",
        "print(report)\n",
        "\n",
        "# Qwen usage ('ollama pull qwen' to download from terminal)\n",
        "print('\\n \\n  Qwen Evaluation')\n",
        "report = evaluate_response(\n",
        "    query=query,\n",
        "    response=response_text,\n",
        "    document=document,\n",
        "    model_type=\"ollama\",\n",
        "    model_name='qwen:latest',\n",
        ")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBtHoutAuDb"
      },
      "source": [
        "### Example with Query-Response Mismatch\n",
        "\n",
        "\n",
        "We replaced the question while keeping the retrieval and document the same to see how the metric evaluates it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "XocqRhE3_3hB",
        "outputId": "4b75f81d-9e4d-4a9a-e27d-5ffdb8df56d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI Evaluation\n",
            "             Metric  Score (Normalized)  Score (%)\n",
            "0   Query Relevance                 0.4       40.0\n",
            "1  Factual Accuracy                 0.8       80.0\n",
            "2          Coverage                 0.4       40.0\n",
            "3         Coherence                 0.6       60.0\n",
            "4           Fluency                 1.0      100.0\n",
            "5  Overall Accuracy                 0.6       60.0\n"
          ]
        }
      ],
      "source": [
        "# Input strings\n",
        "query = 'List the most popular large language models in 2025'\n",
        "response = llm_response\n",
        "document = wikipedia_text\n",
        "\n",
        "# Get evaluation results\n",
        "# OpenAI usage \n",
        "print('OpenAI Evaluation')\n",
        "report = evaluate_response(\n",
        "    query=query,\n",
        "    response=llm_response,\n",
        "    document=wikipedia_text,\n",
        "    model_type=\"openai\",\n",
        "    model_name='gpt-4o',\n",
        ")\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
